{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "---\n",
    "Nice lets get started!\\\n",
    "Before we start we need to make sure we have installed all the packages to our development environment.\\\n",
    "Either create or make sure you python env is active, you see the env near the top right conner of the screen.\\\n",
    "Next open the terminal and run these commands.\n",
    "```\n",
    "pip install numpy  // Powerful tool for working with array's, matrices, and vectors.\n",
    "pip install gymnasium   // The library containing games our agent can play\n",
    "pip install gymnasium[toy-text]   // The sub package containing the we use in this project.\n",
    "```\n",
    "\n",
    "Now we can import the need libraries.\\\n",
    "\\\n",
    "First we are going to grab the gymnasium library as gym, so we can more easily refference it later.\\\n",
    "Next, we import numpy as np. \\\n",
    "Last we grab math, this is built into the Python language and does not need to bee installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Variables\n",
    "---\n",
    "Here we are initiating all the adjustable variable that will be used to train/test the agent.\\\n",
    "In this this cell, we put all the variable we will want to adjust to maximize the agents results.\\\n",
    "\\\n",
    "The only **NON**-Adjustable variable here is rng, which is used to generate a random value between 0-1.\\\n",
    "Of the adjustable variables the only one that doesn't affect the agents prefromance is render,\\\n",
    "it will just display board and the agent as it runs the map.  It will train much slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = True     # If true, Q-vals wont be updated and random actions will not happen\n",
    "render = True           # If true, the board will render for us to watch, but the agent will train slower.\n",
    "is_slippery = False     # If true, the agent will preform a random action 2/3s of time, without epsilon\n",
    "board = ['SFHG','FHFF','FFFF','HFFH'] # S = start / F = frozen ice / H = hole / G = goal\n",
    "\n",
    "episodes = 500                 # Number of times the agent will attempt the game\n",
    "max_steps = 40                 # The number of moves the agent is allocated per episode\n",
    "learning_rate_a = 0.95         # The weight future Q-vals will have one the current state\n",
    "discount_factor_g = 0.98       # The weight of possible outcomes vs the desired outcome will have \n",
    "\n",
    "epsilon = 1                         # The probability of random events - 1 = 100%\n",
    "epsilon_decay_rate = 2 / episodes   # The amount that will be subtracted from epsilon on every episode\n",
    "rng = np.random.default_rng()       # a random val between 0-1, if > epsilon a random action is choosen / NON-ADJUSTABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agents Envrionment\n",
    "---\n",
    "In this section we have 3 different code cells.\\\n",
    "Reason for this is convenience, this way all the edits are made in the cell above.\\\n",
    "Then after makeing any adjustments, re-run the above cell and the marked below for the changes to take affect.\\\n",
    "\\\n",
    "This function will search the board matrix and find the \"G\".\\\n",
    "Once it finds this, it will add the index of \"G\" in that string and add it to row index multipled by 4.\\\n",
    "This will give you its position value, if the board was a grid like a calender and the first day was 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_goal_in_board(board):\n",
    "    for i, row in enumerate(board): \n",
    "        if 'G' in row: return (i*4)+row.index('G')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, using the Gymnasium [Docs](https://gymnasium.farama.org/) initiate the agents environment.\\\n",
    "Note \"board\", \"is_slippery\", and \"render\".\\\n",
    "These are variables above that can be adjusted. There are note to explain what they do.\\\n",
    "\\\n",
    "We also use the fuction to find the position of the goal, and store in a variable to be used later.\\\n",
    "This has to match the board variable and should not be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=board, map_name=\"4x4\", is_slippery=is_slippery, render_mode='human' if render else None)\n",
    "goal = find_goal_in_board(board)\n",
    "print(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the matrix that will contain all of the Q-values for each action in each state.\\\n",
    "It is in its own cell, so when we switch from train to test we wont reset the Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.zeros((env.observation_space.n,env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test The Agent\n",
    "---\n",
    "Theres a lot going on in this cell.\\\n",
    "We are using the Epsilon-Greedy Algorithm to acheive Markov Decision Process.\\\n",
    "Then we are using a unique solution I came up with that maps each states reward in correspondence with its distance from the goal.\\\n",
    "Lastly we use the Bellmans Equation to calculate the Q-value for each action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode 1 lost  in 3 steps\n",
      "0\n",
      "Episode 2 lost  in 3 steps\n",
      "0\n",
      "Episode 3 lost  in 4 steps\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q[state, :])\n\u001b[1;32m---> 14\u001b[0m new_state,reward,terminated,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m new_state \u001b[38;5;241m!=\u001b[39m goal:\n\u001b[0;32m     16\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:308\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:432\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    430\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m    431\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    435\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    436\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "    terminated = False\n",
    "    steps = 0\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "\n",
    "    while(not terminated and steps < max_steps):\n",
    "        if is_training and rng.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q[state, :])\n",
    "\n",
    "        new_state,reward,terminated,_,_ = env.step(action)\n",
    "        if terminated and new_state != goal:\n",
    "            reward = -2\n",
    "        elif not terminated:\n",
    "            sx = (new_state % 4) + 1\n",
    "            sy = math.floor(new_state/4)+1\n",
    "            gx = (goal % 4) + 1\n",
    "            gy = math.floor(goal/4)+1\n",
    "            reward = ((sx+gx)/2+(sy+gy)/2)*1e-5\n",
    "\n",
    "        if is_training:\n",
    "            q[state, action] = q[state, action]+learning_rate_a*(\n",
    "                reward+discount_factor_g*np.max(q[new_state,:])-q[state, action]\n",
    "            )\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if reward > 0 and terminated: \n",
    "            print(f'Episode {i+1} won in {steps} steps')\n",
    "        elif terminated: \n",
    "            print(f'Episode {i+1} lost  in {steps} steps')\n",
    "\n",
    "        state = new_state\n",
    "    epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
