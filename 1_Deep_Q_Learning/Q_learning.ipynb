{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinenforcement Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinenforcement learning, you essentially have and environment and a goal or task.\\\n",
    "Then you will have an agent, or the A.I.\\\n",
    "The Agent will preform an action to the environment, and in return the agent will be given a reward and a new state.\\\n",
    "A state can be something like its position in the environment or anything that can be described by a set of parameters.\\\n",
    "\\\n",
    "Not every action changes the state, and not every action returns a reward.\\\n",
    "None the less, the agent will continue this cycle exploring its environment, looking for better states and better rewards.\\\n",
    "\\\n",
    "Rewards are normally values of +1/-1 or 1/0.\\\n",
    "The agent could get the reward after completing the goal, or it can receive rewards through its process.\\\n",
    "So the plan is, when the agent does something good, it will get a +1.\\\n",
    "If the agent does something wrong it will recive nothing or a -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation\n",
    "---\n",
    "This well be just an introduction to the Bellman equation.\\\n",
    "As the course goes on we'll add more to this equation.\\\n",
    "\\\n",
    "Concepts:\n",
    "- s - State, the state the agent is in, or any state the agent can be in\n",
    "- a - Action, any actions the agent can take\n",
    "- R - Reward, thats the reward the agent will get for entering a certain state\n",
    "- $\\gamma$ - Discount, See below for more on the Discount\n",
    "> ### $V(s)=\\displaystyle\\max_{a}(R(s,a)+\\gamma V(s'))$\n",
    "What we a looking for in this equation is the value of a certain state.\\\n",
    "To best explain the objective let visualize a very basic envirnoment.\\\n",
    "\\\n",
    "![Bellman1](./img/Bellman(1).png)\\\n",
    "\\\n",
    "Here is a very basic maze.\\\n",
    "The green sqaure is the goal with a reward of +1.\\\n",
    "The red sqaure is bad, or gameover, with a reward of -1.\\\n",
    "The black square is a wall, or dead space.\\\n",
    "Finally all the white sqaures currently mean nothing, and have a reward of 0.\\\n",
    "\\\n",
    "Its important to note that all the playable(white) space can have both a reward an a value.\\\n",
    "\\\n",
    "What will happen is the agent will start in the bottom left of the maze and through random actions find its way to the goal.\\\n",
    "Once it finds the goal the agent will look back at how it got there.\\\n",
    "The last playable square that lead the agent to the goal will be assigned the value of 1.\\\n",
    "It will then use the Bellman equation to assign values to all the spaces.\\\n",
    "\n",
    "Looking something like this.\\\n",
    "\\\n",
    "![Bellman2](./img/Bellman(2).png)\\\n",
    "\\\n",
    "Now that we can visualize whats happening, It might be easier to explain.\\\n",
    "We are going to break the equation down peice by peice.\\\n",
    "\\\n",
    "$V(s)$ is refferencing the current state we are trying to solve for.\\\n",
    "$\\displaystyle\\max_{a}$, we know the agent has multiple action per state. We solve for all action and take the max value.\\\n",
    "$R(s,a)$, When the agent preforms an action in the current state its given its reward. Since most states are zero in our case, this is normally 0.\\\n",
    "$\\gamma V(s^{'})$ multiplies the gamma with the value of the state it would be in if it preforms said action.\\\n",
    "This why the agent will learn the environment backwards.\\\n",
    "\\\n",
    "Here we assigned gamma to be 0.9, it doesn't have to be excatly that.\\\n",
    "Whats imortant this of it as a discounter.\\\n",
    "The gamma will \"discount\" the next states value and assign the discounted value to the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Plan\n",
    "---\n",
    "The plan, best put is converting what we now have into a \"map\" for the agent to follow.\\\n",
    "Basically instead of looking at the states value, its replaced with pointers telling the agent where to go next.\\\n",
    "\\\n",
    "![Plan](./img/Plan.png)\\\n",
    "\\\n",
    "Now, when placed any where in the environment the agent will automatically know the best path to its goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "---\n",
    "### Deterministic Search\n",
    "Up until now this is what we have been working with.\\\n",
    "What this means is that with 100% certainty, if the agent were to choose the action of moving up, it would move up.\n",
    "\n",
    "### Non-Deterministic Search\n",
    "Non-Deterministic Search, on the otherhand,\\\n",
    "there is a certain level of probablilty that the agent may choose one action and prefrom a another, putting the agent is a differnet state.\\\n",
    "For example, lets say if an agent wants to move up, there is a small change the agent might actually move left or right.\n",
    "\n",
    "### Markov Process\n",
    "A stochastic model describing a sequence of possible events in which the probability\\\n",
    "of each event depends only on the state attained in the previous event.\\\n",
    "~ [Wikipedia](https://en.wikipedia.org/wiki/Markov_chain)\\\n",
    "What this basically means is the probability of the next action depends only on the current state, not the path that lead to said state.\n",
    "\n",
    "### Markov Decision Processes\n",
    "A Process that provides a mathematical framework for modeling decision making in\\\n",
    "situations where outcomes are partly random and partly under control of the decision maker.\\\n",
    "\\\n",
    "Lets think back to our example in the Bellman equation section.\\\n",
    "Now lets say for any given state, if the agent decides to go up, we can say that there is a 80% chance the agent will preform that action.\\\n",
    "Then we could there a 10% chance the agent will move left and a 10% change the agent will move right.\\\n",
    "This means we have to account for the probability of the agent ending up the in varying states.\\\n",
    "\\\n",
    "We do this by finding the total probable state value of an action.\\\n",
    "Or, multiply the value of each possible future state by the probability of the agent ending in that state.\\\n",
    "Then finding the sum of those new values.\\\n",
    "\\\n",
    "This brings us to the first addition to our Bellman equation.\n",
    "> ##### $0.8*V(s^{'}_{1})+0.1*V(s^{'}_{2})+0.1*V(s^{'}_{3})$ = $\\displaystyle\\sum_{s^{'}}P(s,a,s^{'})$\n",
    "To start, what we see above is the same equation both sides of the = sign.\\\n",
    "The left side of the equation is multiplying the value of each possible state by its respective probability.\\\n",
    "The right side of the equation is a more condensed version of its counterpart.\\\n",
    "The import thing to note is that what we get from this equation is not a probability, but a vaule.\\\n",
    "Just, the value of the next state after you factor in the probability of the agent preforming an action other than the one it chose.\\\n",
    "More or less a weighted average.\\\n",
    "\\\n",
    "Now, we factor in this new probable state value into our Bellman's equation.\n",
    "> #### $V(s) = \\displaystyle\\max_{a}( R(s,a) + \\gamma \\displaystyle\\sum_{s^{'}} P(s,a,s^{'})V(s^{'}))$\n",
    "Ok, so let me try to explain, hopefully I'm right.\\\n",
    "\\\n",
    "The value of this state is equal too...\\\n",
    "The **max** value from ever course of action prefromed in this state...\\\n",
    "The value of a course of action is equal too...\\\n",
    "The Reward of that action on this state **added** to $\\gamma$ **multiplied** by the **sum** of the probable state value and the expected state value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Vs Plan\n",
    "---\n",
    "Like the *Plan*, a *Policy* is like a map for the agent to follow.\\\n",
    "The difference is the *Plan* is a deterministic search, where a states value is based on the best adjacent state.\\\n",
    "The *Policy* is a non-deterministic search, where a states value is based on multiple adjacent states, good and bad.\\\n",
    "\\\n",
    "So how would our *Policy*?\n",
    "\\\n",
    "![Policy](./img/Policy.png)\n",
    "###### Values are not accurate*\n",
    "\\\n",
    "So why did all the values change?\n",
    "When you factor in the change of landing in the wrong states, the current states value will change.\\\n",
    "One of the biggest differences to note is, if a state is adjacent or even near a state with a low value or negative reward,\\\n",
    "the current state we be also be devalued.\\\n",
    "\\\n",
    "For example, in our map, row 2 column 3.\\\n",
    "Before as the agent passed by the state with a -1 Reward, there was 0% chance of the agent moving into that state unless it chose too.\\\n",
    "Now, with our *Policy*, if the agent ends up in that state, there always a chance the agent ends up in the state with a -1 Reward.\\\n",
    "Making that a very risky state to be in.\\\n",
    "\\\n",
    "Which is also why row 1 column 1 also has a lower value than before.\\\n",
    "When the agent is in the this state it always has a chance to fall in row 2 column 3.\\\n",
    "Then the agent has a chance to end in the -1 state again.\\\n",
    "This really illustrates how, with the Markov Decision Process, all the varying states of the enviornment are better connected.\\\n",
    "\\\n",
    "So lets just look at how an agent trained with our new Bellmans Equation would apply its pointers to our *Policy*.\\\n",
    "\\\n",
    "![Policy2](./img/Policy(2).png)\\\n",
    "\\\n",
    "Oh wow! Whats going on with  the two highlighted pointers?\\\n",
    "This is just an example of how impressive A.I. can be.\\\n",
    "An agent that trains in our *Policy* could learn that when in the state row 2 column 3,\\\n",
    "it will have an 80% chance to end up back in its current state, 10% chance it will move up,\\\n",
    "10% chance it will move down, and 0% chance of moving into the red, if the agent decides to move left into the wall.\\\n",
    "\\\n",
    "Like wise, in row 3 column 4, the agent will have a 90% chance to end back in its current state,\\\n",
    "10% chance it will move left, and 0% chance of moving into the red, if the agent decides to move down into the wall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Living Penalty\n",
    "---\n",
    "Up until now the agent in our example has only been receiving a Reward at the end of the game.\\\n",
    "Either +1 for the win or -1 for game over, but in reality the agent will normally receive Rewards along the way.\\\n",
    "Lets say that all the states that currently have a Reward of 0 will now have a Reward of -0.04.\\\n",
    "\\\n",
    "![LivingPenalty1](./img/LivingPenalty(1).png)\\\n",
    "\\\n",
    "Before our agent was moving arround, unconcered about its number of moves or time spent navigating the maze.\\\n",
    "The agent will get a negative Reward every time it preforms a new action, except when it moves to the green state.\\\n",
    "This will encourage the agent to want to finish the game in less moves.\\\n",
    "How will imposing a negative reward affect our policy?\\\n",
    "\\\n",
    "Here a few examples\\\n",
    "\\\n",
    "![LivingPenalty2](./img/LivingPenalty(2).png)\\\n",
    "\\\n",
    "Here we can see, when there is 0 penalty, the agent will not take any risks to reach its goal.\\\n",
    "In the 2nd example, with a small penalty, the agent still prefers to avoid risk, but will take small risks to save moves.\\\n",
    "The 3rd example, is starting to chose a riskier path in order to reduce the number of moves to a minimum.\\\n",
    "In the final, where the Reward is even less than the Reward in the red state,\\\n",
    "the agent will do anything to end the game as soon as possible.\\\n",
    "Even if it means chosing to step in the red state and forefit the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "---\n",
    "So up until now we have been working \"V\", or the value of a specific state.\\\n",
    "Now we are going to make another adjustment to our Bellmans equation.\\\n",
    "Instead of solving for \"V\" to find the value of a state, we are going to solve for \"Q\" to find the quality of an action.\\\n",
    "\\\n",
    "One difference to note is that before we measured the value of possible future state, and was not concerned with how the agent got there.\\\n",
    "The agent will now evauluate the qaulity of every action, regaurdless of where the agent came from.\n",
    "\\\n",
    "Ok lets talk about our equation.\\\n",
    "Lets start by revisting the what we have so far.\n",
    "> #### $V(s) = \\displaystyle\\max_{a}( R(s,a)+\\gamma\\displaystyle\\sum_{s^{'}}P(s,a,s^{'})V(s^{'}) )$\n",
    "Basically the value of a state is equal to the maxium value of all possible future states.\\\n",
    "In other words, to solve for \"V\" you need to know all the values of future \"V\"s.\\\n",
    "Making this a recursive fuction, this will play a role in just a moment.\\\n",
    "\\\n",
    "So how do we find the *quality* of the action?\\\n",
    "Well, to start, remember before we were finding the value of a state.\\\n",
    "From a state, with the randomness of the Markov Process, the agent could up in preforming any action.\\\n",
    "Now putting value on the action is defferent, because once the action is made you know the state you'll be in.\\\n",
    "Still, finding the Q-value of a action is no different than finding the value of a state.\\\n",
    "To know the quailty of an action you would the to know the value of all future moves.\\\n",
    "\\\n",
    "Lets have a look.\n",
    "> #### $Q(s,a) = R(s,a) + \\gamma \\displaystyle\\sum_{s^{'}}(P(s,a,s^{'})V(s^{'}))$\n",
    "OK, so this is saying,\\\n",
    "the Q-value of this is action **equal** too...,\\\n",
    "the Reward **added** too..\\\n",
    "$\\gamma$ **multiplied** by...\\\n",
    "the sum of probable value and the expect value.\\\n",
    "\\\n",
    "Remember how the equation before was recursive?\\\n",
    "The same holds true here.\\\n",
    "instead of evaulating the Q-value of the current action based on the values of future states,\\\n",
    "we need to evaluate the Q-value of an action based on future action qualities.\\\n",
    "Making the formula recursive once again.\\\n",
    "\\\n",
    "But, how do we do that?\\\n",
    "Well, as we seen above, essentually $Q(s,a)=\\displaystyle\\max_{a}(V(s^{'}))$\\\n",
    "So what we need to do is replace the $V(s^{'})$ with the max value of $Q(s^{'},a^{'})$\n",
    "> #### $Q(s,a) = R(s,a) + \\gamma \\displaystyle\\sum_{s^{'}}(P(s,a,s^{'})\\displaystyle\\max_{a^{'}}Q(s^{'},a^{'}))$\n",
    "This is now are new formula, and we only have one more change to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperal Difference\n",
    "---\n",
    "There's just one more component to consider now.\\\n",
    "As the agent is training, through its many iterations of exploring the environment, it's likely to land in the same state many times.\\\n",
    "It would make sense that the agent will update the Q-value on every passing of that state.\\\n",
    "Which is what we wanted, as it explores all actions in that state, it learns the best action.\\\n",
    "Once it knows the best action, it will always chose this action, reinforcing its learning.\\\n",
    "\\\n",
    "While this is good, we know that there is a random chance the agent will chose the best action and end in a different state.\\\n",
    "So the agent will recalculate the Q-value of that aciton in that state with a lower value.\\\n",
    "That's Very Temperal Differnce comes in.\\\n",
    "\\\n",
    "So as we know, as the agent explores it starts to establish the Q-values.\\\n",
    "Once the agent passes through a state, that Q-vaule is store in memory for next time.\\\n",
    "When the agent comes to that state again, it run an action and re-calculate a new Q-value.\\\n",
    "For a moment the will be two Q-values, only difference being time.\\\n",
    "The Q-value in memory, or previous Q-value( $t-1$ ), and the new Q-value( $t$ ).\\\n",
    "\\\n",
    "So to actually find the Temperal Difference, we simply subtract the previous Q-value( $t-1$ ) from the Bellman equation.\n",
    "> #### $ TD(a,s) = R(s,a) + \\gamma \\displaystyle\\sum_{s^{'}}(P(s,a,s^{'}) \\displaystyle\\max_{a^{'}} Q(s^{'},a^{'})) - Q_{t-1}(s,a) $\n",
    "Ok thats a lot, although thats our new equation, we are going to do something different for the next few examples.\\\n",
    "Remember the section about deterministic and non-deterministic searches, and adding $\\displaystyle\\sum_{s^{'}}P(s,a,s^{'})$\\\n",
    "for the next few examples I will write in a deterministc fomula, but know that the Markov Process will be used.\\\n",
    "Also, sometimes in literature, this example will be used for explaining Temperal Difference.\\\n",
    "\\\n",
    "So we will be looking at this instead.\n",
    "> #### $ TD(a,s) = R(s,a) + \\gamma \\displaystyle\\max_{a^{'}} Q(s^{'},a^{'}) - Q_{t-1}(s,a) $\n",
    "Ok, so we have our Temperal Difference. What do we do with it?\\\n",
    "We are going to add the Temperal Difference from the new calculation to the previous Q-value to get out new Q-value.\\\n",
    "But first, we need to introduce a new parameter, $\\alpha$, or the learning rate.\\\n",
    "So once we have $TD$, we can find the new Q-value.\n",
    "> #### $ Q_{t}(s,a) = Q_{t-1}(s,a) + \\alpha TD_{t}(a,s) $\n",
    "So really our equation is actually even bigger.\n",
    "> #### $ Q_{t}(s,a) = Q_{t-1}(s,a) + \\alpha ( R(s,a) + \\gamma \\displaystyle\\max_{a^{'}} Q(s^{'},a^{'}) - Q_{t-1}(s,a) ) $\n",
    "Now we can better see the role $\\alpha$ will play.\\\n",
    "The best way to point this out is if we look, we see a positive $Q_{t-1}(s,a)$ and a negative $-Q_{t-1}(s,a)$.\\\n",
    "$\\alpha$ will only have an affect on the negative $-Q_{t-1}(s,a)$.\\\n",
    "Meaning if $\\alpha$ is set to 0, the $Q_{t-1}(s,a)$ will negated, and the Q-value will be completely replaced. There will be no Temperal Difference\\\n",
    "While if $\\alpha$ is set to 1, the entire $R(s,a)+\\gamma\\displaystyle\\max_{a^{'}}Q(s^{'},a^{'})-Q_{t-1}(s,a)$ is negated, \n",
    "and the Q-value will never change.\\\n",
    "\\\n",
    "So, ideally we will set the learning rate, $\\alpha$, to a values between 0 and 1.\n",
    "> #### $ Q_{t}(s,a) = Q_{t-1}(s,a) + \\alpha ( R(s,a) + \\gamma \\displaystyle\\sum_{s^{'}}(P(s,a,s^{'}) \\displaystyle\\max_{a^{'}} Q(s^{'},a^{'})) - Q_{t-1}(s,a) ) $\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
