{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning ( Not Working )\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Environment and Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install numpy\n",
    "pip install gymnasium\n",
    "pip install \"gymnasium[atari,accept-rom-license]\"\n",
    "\n",
    "NOTE: I am on Windows 10 with Nvidia 1050GTX GPU / you may need something different for a new system \n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a new envrionment with venv for this project.\\\n",
    "We do this using VSCode by selecting the evnironment (might say kernal or Python), near the top right corner of the IDE.\\\n",
    "We will click \"Select Another Kernal.../Python Environments.../+ Create Python Environment\".\\\n",
    "We will need to name the environment.\\\n",
    "We use VSCode, so we can easily conncet our environment to our notebook and run the code blocks right here in our notebook.\\\n",
    "\\\n",
    "Once we have the Envrionment we install the packages we need.\\\n",
    "Gymnasium is a library that provides us with learning playground to build A.I. in.\\\n",
    "Gymnasium[atari,accept-rom-license] will give us access to a list of Atari games we can train our agents to play. ( Later in the course )\\\n",
    "Then we install Py-Torch, you will want to visit their site and fine the proper command for your system.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the A.I.\n",
    "### Create The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed = 42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 28)\n",
    "        self.fc2 = nn.Linear(28, 28)\n",
    "        self.fc3 = nn.Linear(28, 28)\n",
    "        self.fc4 = nn.Linear(28, 28)\n",
    "        self.fc5 = nn.Linear(28, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The A.I.\n",
    "### Setup The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Shape: (2,)\n",
      "State Size: 2\n",
      "Number of Actions: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "envs = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "state_shape = envs.observation_space.shape\n",
    "state_size = envs.observation_space.shape[0]\n",
    "num_of_actions = envs.action_space.n\n",
    "print(f'''\n",
    "State Shape: {state_shape}\n",
    "State Size: {state_size}\n",
    "Number of Actions: {num_of_actions}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Hyperparametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .9 # alpha, 0.0005\n",
    "mini_batch_size = 100 # number of observations used per step to update the model.\n",
    "discount_factor = 0.9 # gamma - closer to one the more conseration there is of future rewards\n",
    "replay_buffer_size = int(1e5) # For experience Replay, number of observations in AI memory, 100,000\n",
    "interpolation_parameter = 1e-3 # 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory, k = batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        return states, next_states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize The DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
    "        self.memory = ReplayMemory(replay_buffer_size)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > mini_batch_size:\n",
    "                experiences = self.memory.sample(100)\n",
    "                self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon = 0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_qnetwork(state)\n",
    "        self.local_qnetwork.train()\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, next_states, actions, rewards, dones = experiences\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, interpolation_parameter):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize The DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, num_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -100.00\n",
      "Episode 200\tAverage Score: -100.00\n",
      "Episode 300\tAverage Score: -100.00\n",
      "Episode 400\tAverage Score: -100.00\n",
      "Episode 500\tAverage Score: -100.00\n",
      "Episode 600\tAverage Score: -100.00\n",
      "Episode 668\tAverage Score: -100.00"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "number_episodes = 1000\n",
    "maximum_number_timesteps_per_episode = 100\n",
    "epsilon_starting_value  = 1.0\n",
    "epsilon_ending_value  = 0.01\n",
    "epsilon_decay_value  = 0.02\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "    state, _ = envs.reset()\n",
    "    score = 0\n",
    "    for t in range(maximum_number_timesteps_per_episode):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, done, _, _ = envs.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    scores_on_100_episodes.append(score)\n",
    "    epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
    "    if episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
    "    if np.mean(scores_on_100_episodes) >= 200.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
    "        torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\Documents\\Artificial Intelligence\\1_Deep_Q_Learning\\practical.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     imageio\u001b[39m.\u001b[39mmimsave(\u001b[39m'\u001b[39m\u001b[39mvideo.mp4\u001b[39m\u001b[39m'\u001b[39m, frames, fps\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m show_video_of_model(agent, \u001b[39m'\u001b[39;49m\u001b[39mMountainCar-v0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_video\u001b[39m():\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     mp4list \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m*.mp4\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mg:\\Documents\\Artificial Intelligence\\1_Deep_Q_Learning\\practical.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     frame \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     frames\u001b[39m.\u001b[39mappend(frame)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/Documents/Artificial%20Intelligence/1_Deep_Q_Learning/practical.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state)\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RenderFrame \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[RenderFrame] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mg:\\Documents\\Artificial Intelligence\\.venv\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:229\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m     c \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mVector2(c)\u001b[39m.\u001b[39mrotate_rad(math\u001b[39m.\u001b[39mcos(\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m pos))\n\u001b[0;32m    222\u001b[0m     coords\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    223\u001b[0m         (\n\u001b[0;32m    224\u001b[0m             c[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m (pos \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_position) \u001b[39m*\u001b[39m scale,\n\u001b[0;32m    225\u001b[0m             c[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m clearance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_height(pos) \u001b[39m*\u001b[39m scale,\n\u001b[0;32m    226\u001b[0m         )\n\u001b[0;32m    227\u001b[0m     )\n\u001b[1;32m--> 229\u001b[0m gfxdraw\u001b[39m.\u001b[39;49maapolygon(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf, coords, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m    230\u001b[0m gfxdraw\u001b[39m.\u001b[39mfilled_polygon(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, coords, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m    232\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m [(carwidth \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m0\u001b[39m), (\u001b[39m-\u001b[39mcarwidth \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m0\u001b[39m)]:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "    env.close()\n",
    "    imageio.mimsave('video.mp4', frames, fps=30)\n",
    "\n",
    "show_video_of_model(agent, 'MountainCar-v0')\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
