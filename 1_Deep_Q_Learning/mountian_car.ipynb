{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Mountain Car\n",
    "---\n",
    "In this game, the agent is an underpowered car.\\\n",
    "It can't generate enough torque to get moving, but not climb the hill.\\\n",
    "The object is for the agent to learn to use both hills and momentum to reach the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip Requirements\n",
    "```\n",
    "pip install gymnasium torch numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "\n",
    "capacity = 100000\n",
    "sample_size = 10\n",
    "\n",
    "learning_rate = .000009\n",
    "discount_factor = 0.98\n",
    "interpolation_parameter = 1e-3\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay_rate = 1.2 / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = './models/DQN_mountain_car(3).pth'\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=None)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed=42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = seed\n",
    "        self.fc1 = nn.Linear(state_size, 640)\n",
    "        self.fc2 = nn.Linear(640,1228)\n",
    "        self.fc3 = nn.Linear(1228,1228)\n",
    "        self.fc4 = nn.Linear(1228,640)\n",
    "        self.fc5 = nn.Linear(640, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        experiences = random.sample(self.memory, sample_size)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "        terminations = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        return states, next_states, actions, rewards, terminations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_network = Network(state_size, action_size).to(self.device)\n",
    "        self.target_network = Network(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.local_network.parameters(), lr = learning_rate)\n",
    "        self.acc = accuracy = MulticlassAccuracy(num_classes=3).to(self.device)\n",
    "        self.memory = Memory(capacity)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, terminated):\n",
    "        self.memory.push((state,action,reward,next_state,terminated))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > capacity*0.05:\n",
    "                experiences = self.memory.sample(sample_size)\n",
    "                self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        st = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.local_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.local_network(st)\n",
    "        self.local_network.train()\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action.cpu().data.numpy())\n",
    "        else:\n",
    "            if state[1] > 0:\n",
    "                action = random.choices(np.arange(self.action_size),weights=(20,20,60))\n",
    "            else:\n",
    "                action = random.choices(np.arange(self.action_size),weights=(60,20,20))\n",
    "            return action[0]\n",
    "\n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, next_states, actions, rewards, terminations = experiences\n",
    "        next_q_targets = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + discount_factor * next_q_targets * (1 - terminations)\n",
    "        q_expected = self.local_network(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.local_network, self.target_network, interpolation_parameter)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, interpolation_parameter):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n",
    "\n",
    "    def accuracy(self, e):\n",
    "        X = self.local_network(torch.from_numpy(np.vstack([m[0] for m in agent.memory.memory if m is not None])).to(self.device))\n",
    "        y = self.target_network(torch.from_numpy(np.vstack([m[3] for m in agent.memory.memory if m is not None])).to(self.device))\n",
    "        actions = torch.from_numpy(np.vstack([m[1] for m in agent.memory.memory if m is not None])).to(self.device)\n",
    "        t_acc = self.acc(X, torch.max(y, dim=1)[1])\n",
    "        t_loss = F.mse_loss(X.gather(1, actions), y.detach().max(1)[0].unsqueeze(1))\n",
    "        acc_hist.append(float(t_acc))\n",
    "        loss_hist.append(float(t_loss))\n",
    "        print(f'Episode: {e}\\tLoss:{t_loss}\\tAccuracy: {round(float(t_acc)*100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Agent\n",
    "Get its own cell to avoid re-initializing agent and dumping its memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size,action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Agent\n",
    "To see the un-trained agent, play the next cell first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = epsilon_start\n",
    "\n",
    "try: \n",
    "    best_model = torch.load(FILE)\n",
    "except:\n",
    "    best_model = {'best': 0}\n",
    "\n",
    "for e in range(1, episodes+1):\n",
    "    terminated = False\n",
    "    state,_ = env.reset()\n",
    "\n",
    "    while(not terminated):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            reward = 10\n",
    "        elif next_state[1] > 0:\n",
    "            if next_state[0] < 0:\n",
    "                reward = (next_state[0]*-1)*next_state[1]\n",
    "        elif next_state[1] < 0 and next_state[0] < 0:\n",
    "                reward = ((next_state[0]*1)*next_state[1])*0.1\n",
    "        else:\n",
    "            reward = reward - next_state[0]*next_state[1]\n",
    "        agent.step(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "    epsilon = max(0.05, epsilon - epsilon_decay_rate)\n",
    "    if len(agent.memory.memory) > sample_size:\n",
    "        agent.accuracy(e)\n",
    "    if epsilon <= 0.1 and acc_hist[-1] > 0.0 and acc_hist[-1] < 1.0:\n",
    "        loss_acc = (acc_hist[-1]) / loss_hist[-1]\n",
    "        if loss_acc > best_model['best']:\n",
    "            best_model = {\n",
    "                'best': loss_acc,\n",
    "                'local_network': agent.local_network.state_dict(),\n",
    "                'target_network': agent.target_network.state_dict()\n",
    "            }\n",
    "            torch.save(best_model, FILE)\n",
    "            print('Best set')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.7), layout='constrained')\n",
    "plt.plot(loss_hist, label='loss')\n",
    "plt.plot(acc_hist, label='acc')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('...')\n",
    "plt.title(\"Training Results\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=2, out_features=640, bias=True)\n",
       "  (fc2): Linear(in_features=640, out_features=1228, bias=True)\n",
       "  (fc3): Linear(in_features=1228, out_features=1228, bias=True)\n",
       "  (fc4): Linear(in_features=1228, out_features=640, bias=True)\n",
       "  (fc5): Linear(in_features=640, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.local_network.load_state_dict(torch.load(FILE)['local_network'])\n",
    "agent.local_network.eval()\n",
    "agent.target_network.load_state_dict(torch.load(FILE)['target_network'])\n",
    "agent.target_network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agent And Visualize Results\n",
    "All training and learning methods have been removed.\\\n",
    "Only a 10% of a weighted random event remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1\tScore: 723.0\n",
      "Episode:2\tScore: 766.0\n",
      "Episode:3\tScore: 703.0\n",
      "Episode:4\tScore: 730.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "\n",
    "for i in range(1,5):\n",
    "    terminated = False\n",
    "    state,_ = env.reset()\n",
    "    score = 1000\n",
    "\n",
    "    while(not terminated and score > 0):\n",
    "        action = agent.act(state, .01)\n",
    "        next_state, reward, terminated,_,_ = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    print(f'Episode:{i}\\tScore: {score}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
