{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Mountain Car\n",
    "---\n",
    "In this game, the agent is an underpowered car.\\\n",
    "It can't generate enough torque to get moving, but not climb the hill.\\\n",
    "The object is for the agent to learn to use both hills and momentum to reach the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip Requirements\n",
    "```\n",
    "pip install gymnasium torch numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "\n",
    "capacity = 10000\n",
    "sample_size = 10\n",
    "\n",
    "learning_rate = .00001\n",
    "discount_factor = 0.9\n",
    "interpolation_parameter = 1e-3\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay_rate = 1.2 / episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=None)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed=42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = seed\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128,636)\n",
    "        self.fc3 = nn.Linear(636,64)\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        experiences = random.sample(self.memory, sample_size)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "        terminations = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        return states, next_states, actions, rewards, terminations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_network = Network(state_size, action_size).to(self.device)\n",
    "        self.target_network = Network(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.local_network.parameters(), lr = learning_rate)\n",
    "        self.memory = Memory(capacity)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, terminated):\n",
    "        self.memory.push((state,action,reward,next_state,terminated))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > sample_size:\n",
    "                experiences = self.memory.sample(sample_size)\n",
    "                self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        st = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.local_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.local_network(st)\n",
    "        self.local_network.train()\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action.cpu().data.numpy())\n",
    "        else:\n",
    "            if state[1] > 0:\n",
    "                action = random.choices(np.arange(self.action_size),weights=(20,20,60))\n",
    "            else:\n",
    "                action = random.choices(np.arange(self.action_size),weights=(60,20,20))\n",
    "            return action[0]\n",
    "\n",
    "    def learn(self, experiences, discount_factor):\n",
    "        states, next_states, actions, rewards, terminations = experiences\n",
    "        next_q_targets = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + discount_factor * next_q_targets * (1 - terminations)\n",
    "        q_expected = self.local_network(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.local_network, self.target_network, interpolation_parameter)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, interpolation_parameter):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Agent\n",
    "Get its own cell to avoid re-initializing agent and dumping its memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size,action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Agent\n",
    "To see the un-trained agent, play the next cell first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = epsilon_start\n",
    "best = float('-inf')\n",
    "\n",
    "for e in range(1, episodes+1):\n",
    "    terminated = False\n",
    "    state,_ = env.reset()\n",
    "    score = 1000\n",
    "\n",
    "    while(not terminated and score > 0):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            reward = 10\n",
    "        elif next_state[1] > 0:\n",
    "            if next_state[0] < 0:\n",
    "                reward = (next_state[0]*-1)*next_state[1]\n",
    "        elif next_state[1] < 0 and next_state[0] < 0:\n",
    "                reward = ((next_state[0]*1)*next_state[1])*0.1\n",
    "        else:\n",
    "            reward = reward - next_state[0]*next_state[1]\n",
    "        agent.step(state, action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    epsilon = max(0.05, epsilon - epsilon_decay_rate)\n",
    "    best = max(best, score)\n",
    "    if e % 2 == 0:\n",
    "        print(f'Episode: {e}\\tScore: {score}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agent And Visualize Results\n",
    "All training and learning methods have been removed.\\\n",
    "Only a 10% of a weighted random event remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "\n",
    "for i in range(1,5):\n",
    "    terminated = False\n",
    "    state,_ = env.reset()\n",
    "    score = 1000\n",
    "\n",
    "    while(not terminated and score > 0):\n",
    "        action = agent.act(state, .1)\n",
    "        next_state, reward, terminated,_,_ = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    print(f'Episode:{i}\\tScore: {score}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
