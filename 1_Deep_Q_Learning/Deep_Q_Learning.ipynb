{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "---\n",
    "### Learning\n",
    "Deep Q-learning is taking the material covered in [Q-learning](./Q_learning.ipynb) and combining it with the material\\\n",
    "covered in [ANN](../../Deep%20Learning/Supervised_Learning/ANN/ann.ipynb) ( Artificial Neural Networks ).\\\n",
    "With that said the material covered in this section will be presented with the understanding of those two additional sections.\\\n",
    "\\\n",
    "So where we left off with Q-learning is our agent will explore the map and assign Q-values to each action for each state using the Bellmans Equation.\\\n",
    "We also know that using these Q-values with other parameters, like reward, the agent will choose an action based on these values.\\\n",
    "\\\n",
    "Using Artificial Neural Networks for Deep Q-Learnng works simular, but leverages Deep leaning for Q-Learning.\\\n",
    "Lets, look back to the example from the Q-learning section.\\\n",
    "We can put our environment on a vertical/horzontal axis.\\\n",
    "Only instead of x,y we'll think of it as $X_{1}$ and $X_{2}$.\\\n",
    "\\\n",
    "![mapToANN](./img/DeepQ(1).png)\\\n",
    "\\\n",
    "$X_{1}$ and $X_{2}$ would contain all the data of that state and we could pass it the ANN.\\\n",
    "As we recall, when we calculate our Temporal Difference we work with the Q value in 2 points in time.\\\n",
    "The ANN will as well, it will store all four Q-value in memory.\\\n",
    "The next time the agent lands in that state all four Q-values from the previous iteration will be compared to their new Q-Values.\\\n",
    "Where we know it will calculate the loss of its action.\n",
    "> #### $L = \\sum(QTarget-target)^{2}$\n",
    "Then it back propagate, and adjust the weights to decreace its loss.\\\n",
    "That would be comparable to adjusting the $\\alpha$ and $\\gamma$ to decrease the Temporal Difference.\n",
    "### Acting\n",
    "Based on these comparisons and the weights of the connections between nodes, the model will produce a Q-value for each action.\\\n",
    "We then will apply an action selection policy the will pick the height Q-value for us.\\\n",
    "The softmax in our case.\n",
    "## Experience Replay\n",
    "When our agent starts training, unlike before in Deep Learning, we don't have a large dataset to train the agent with.\\\n",
    "Instead the agent will start with no data, and will have to record data from its own learned experiences, this is one problem.\\\n",
    "Another issue we have is that, it is common for an environment many consecutive states where multiple parameters stay the same.\\\n",
    "Causing the agent to do one thing very well, but it will not know what to do when the environment changes.\\\n",
    "\\\n",
    "For example, imagine a self driving car driving a long, straight, empty road.\\\n",
    "The way we update the weights of the neural network with every state, The car will learn how to drive that straight line exceptionally well.\\\n",
    "If the road were to then turn the car is likely to not know how to respond.\\\n",
    "Even if car some made the turn, on the next straight the neural network wolud restructure itself for the new straight road.\\\n",
    "\\\n",
    "These issues are addresed by allowing the agent to run and collect datapoints on the environment until it meets a certain threshold.\\\n",
    "Once the threshold is met the agent will start to tain.\\\n",
    "The was the agent will train is simular to the way we trained our learning models in Deep Learning, with batch sizes.\\\n",
    "Difference is that in Deep Learning the model would get batches of random data points,\\\n",
    "With Experience Replay we be getting a uniformly distributed sample.\\\n",
    "\\\n",
    "This means the agent will grab datapoints evenly from across the entire dataset.\\\n",
    "Another benifit to having uniformly distributed sample, is the agent will be able learn from an recall rare events.\\\n",
    "## Action Selction Policies\n",
    "If we remember from Q-learning, the Markov Decision Process is when the is partially in control and partially not in control of its actions.\\\n",
    "We do this to force the agent to always continue exploring, rather find a local maxium.\\\n",
    "For instance, lets say an agent has 4 possible actions in a state, and it has explored 3 of them.\\\n",
    "One of which gives the agent a good reward that it likes, although the forth action would be better.\\\n",
    "The agent could end up continuing to choose the second best action and miss the best action entirely.\\\n",
    "\\\n",
    "There are many Action selection polices available.\\\n",
    "Some of the more common ones are:\n",
    "- $\\varepsilon$-greedy / Will select the best Q-value except *epsilon* % of the time, at which point it will be slected randomly\n",
    "- $\\varepsilon$-soft(1-$\\varepsilon$) / Will select the best Q-value 1-*epsilon* % of the time, and the rest will be random\n",
    "- Softmax / Takes all outputs and \"squashes\" them to proportional values between 0-1, adding up to 1, giving probablities\n",
    "\n",
    "As already mentioned, we will use Softmax.\\\n",
    "Instead of setting a prefixed amount of complete random selection of an action, softmax gives us percentages.\\\n",
    "Percentages that can be used to decide how often the agent will choose the that action.\\\n",
    "As the agent becomes more or less \"confident\" in that action, that outputs percentage will go up or down.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
