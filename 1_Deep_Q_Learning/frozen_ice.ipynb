{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "---\n",
    "Nice lets get started!\\\n",
    "Before we start we need to make sure we have installed all the packages to our development environment.\\\n",
    "Either create or make sure you python env is active, you see the env near the top right conner of the screen.\\\n",
    "Next open the terminal and run these commands.\n",
    "```\n",
    "pip install numpy  // Powerful tool for working with array's, matrices, and vectors.\n",
    "pip install gymnasium   // The library containing games our agent can play\n",
    "pip install gymnasium[toy-text]   // The sub package containing the we use in this project.\n",
    "```\n",
    "\n",
    "Now we can import the need libraries.\\\n",
    "\\\n",
    "First we are going to grab the gymnasium library as gym, so we can more easily refference it later.\\\n",
    "Next, we import numpy as np. \\\n",
    "Last we grab math, this is built into the Python language and does not need to bee installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False     # If true, Q-vals wont be updated and random actions will not happen\n",
    "render = True           # If true, the board will render for us to watch, but the agent will train slower.\n",
    "is_slippery = False           # If true, the agent will preform a random action 2/3s of time, without epsilon\n",
    "\n",
    "episodes = 5                   # Number of times the agent will attempt the game\n",
    "learning_rate_a = 0.95         # The weight future Q-vals will have one the current state\n",
    "discount_factor_g = 0.98       # The weight of possible outcomes vs the desired outcome will have \n",
    "\n",
    "epsilon = 1                         # The probability of random events - 1 = 100%\n",
    "epsilon_decay_rate = 2 / episodes   # The amount that will be subtracted from epsilon on every episode\n",
    "rng = np.random.default_rng()       # a random val between 0-1, if > epsilon a random action is choosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agents Envrionment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=['SFHH','FFFF','HHFF','GFFH'], map_name=\"4x4\", is_slippery=is_slippery, render_mode='human' if render else None)\n",
    "goal = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the matrix that will contain all of the Q-values for each action in each state.\\\n",
    "It is in its own cell, so when we switch from train to test we wont reset the Q-values.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.zeros((env.observation_space.n,env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test The Agent\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 won in 7 steps\n",
      "Episode 2 won in 7 steps\n",
      "Episode 3 won in 7 steps\n",
      "Episode 4 won in 7 steps\n",
      "Episode 5 won in 7 steps\n"
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "    terminated = False\n",
    "    steps = 0\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "\n",
    "    while(not terminated and steps < 30):\n",
    "        if is_training and rng.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q[state, :])\n",
    "\n",
    "        new_state,reward,terminated,_,_ = env.step(action)\n",
    "        if terminated and new_state != goal:\n",
    "            reward = -2\n",
    "        elif not terminated:\n",
    "            sx = (new_state % 4) + 1\n",
    "            sy = math.floor(new_state/4)+1\n",
    "            gx = (goal % 4) + 1\n",
    "            gy = math.floor(goal/4)+1\n",
    "            reward = ((sx+gx)/2+(sy+gy)/2)*1e-5\n",
    "\n",
    "        if is_training:\n",
    "            q[state, action] = q[state, action]+learning_rate_a*(\n",
    "                reward+discount_factor_g*np.max(q[new_state,:])-q[state, action]\n",
    "            )\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if reward > 0 and terminated: \n",
    "            print(f'Episode {i+1} won in {steps} steps')\n",
    "        elif terminated: \n",
    "            print(f'Episode {i+1} lost  in {steps} steps')\n",
    "\n",
    "        state = new_state\n",
    "    epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
